{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "127136b7-af9a-43eb-a89d-361cf709ae43",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f6795b-b1c5-433d-b704-2291088f339f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import tensorflow as tf\n",
    "# import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer, Conv2D, Dense, MaxPooling2D, Input, Flatten\n",
    "from tensorflow.keras.metrics import Precision, Recall, BinaryAccuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a836ff4e-5278-413d-bbe5-54710bb1312b",
   "metadata": {},
   "source": [
    "# Preprocess (Load - Resize - Scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000173cc-1617-4a8f-b123-3013dc295849",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_pairs(file_path):\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        pairs, labels = pkl.load(f)\n",
    "    return pairs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4de5af-9f2a-463d-9e86-8bf64b6875b5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# file_path = 'pairs/train_pairs_mixed.pkl'\n",
    "# pairs , labels = load_pairs(file_path)\n",
    "# print(f'pairs shape : {pairs.shape} , labels shape : {labels.shape} ')\n",
    "# first_pair = pairs[0]\n",
    "# first_label = labels[0]\n",
    "# print(f'first pair : {first_pair}')\n",
    "# print(f'first label : {first_label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1eceaeb-869e-453d-a558-324fcd4e5f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_scale(img_path):\n",
    "    byte_img = tf.io.read_file(img_path)\n",
    "    img = tf.io.decode_jpeg(byte_img)\n",
    "    img = tf.image.resize(img, (100,100))\n",
    "    img = img / 255.0\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f76b3c8-4b93-41d4-87f0-85761ade5481",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# img_path = first_pair[0]\n",
    "# img = resize_scale(img_path)\n",
    "# print(img)\n",
    "# plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3368964-7b4c-45e1-a1c3-1c8c04e6ad45",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_data(file_path):\n",
    "    pairs, labels = load_pairs(file_path)\n",
    "    imgs_left = []\n",
    "    imgs_right = []\n",
    "    for i in range(len(pairs)):\n",
    "        imgs_left.append(resize_scale(pairs[i][0]))\n",
    "        imgs_right.append(resize_scale(pairs[i][1]))\n",
    "    imgs_left = tf.convert_to_tensor(imgs_left)\n",
    "    imgs_right = tf.convert_to_tensor(imgs_right)\n",
    "    labels = tf.convert_to_tensor(labels)\n",
    "    data = tf.data.Dataset.from_tensor_slices((imgs_left, imgs_right, labels))\n",
    "    data = data.cache().shuffle(buffer_size=10000, seed=42)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5061e5b1-8592-4495-927e-0d5ad64c12bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = preprocess_data('pairs/train_pairs_mixed.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f0f05a-b221-44c0-9577-f78d986f3b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f' train_data length : {len(train_data)}')\n",
    "# samples = train_data.as_numpy_iterator()\n",
    "# example = samples.next()\n",
    "# print(f'first data : {example}')\n",
    "# plt.imshow(example[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5def36-561c-4475-82c2-b466c09b5ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = preprocess_data('pairs/validation_pairs_mixed.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d4cdc1-da27-4e89-b197-bb224612ba03",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = preprocess_data('pairs/test_pairs_mixed.pkl')\n",
    "test_data_men = preprocess_data('pairs/test_pairs_men.pkl')\n",
    "test_data_women = preprocess_data('pairs/test_pairs_women.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31af89b-c645-47fa-953d-1519c1b028aa",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea228bf1-091a-4d26-ba19-1db224267532",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_embedding(): \n",
    "    inp = Input(shape=(100,100,3), name='input_image')\n",
    "    \n",
    "    c1 = Conv2D(64, (10,10), activation='relu')(inp)\n",
    "    m1 = MaxPooling2D(64, (2,2), padding='same')(c1)\n",
    "    \n",
    "    c2 = Conv2D(128, (7,7), activation='relu')(m1)\n",
    "    m2 = MaxPooling2D(64, (2,2), padding='same')(c2)\n",
    "    \n",
    "    c3 = Conv2D(128, (4,4), activation='relu')(m2)\n",
    "    m3 = MaxPooling2D(64, (2,2), padding='same')(c3)\n",
    "    \n",
    "    c4 = Conv2D(256, (4,4), activation='relu')(m3)\n",
    "    f1 = Flatten()(c4)\n",
    "    d1 = Dense(4096, activation='sigmoid')(f1)\n",
    "    \n",
    "    return Model(inputs=inp, outputs=d1, name='embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e1e402-b46a-462d-a5df-5de03b35e068",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = make_embedding()\n",
    "# embedding.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42eb151b-b144-4e13-ab27-cefcb18a4b14",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class L1Dist(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "    def call(self, embedding_left, embedding_right):\n",
    "        return tf.math.abs(embedding_left - embedding_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd2fbdc-a45c-40af-b3b5-a5835ab6e0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_siamese_model(): \n",
    "    \n",
    "    image_left = Input(name='image_left', shape=(100,100,3))\n",
    "    image_right = Input(name='image_right', shape=(100,100,3))\n",
    "\n",
    "    embedding_left = embedding(image_left)\n",
    "    embedding_right = embedding(image_right)\n",
    "\n",
    "    dist_layer = L1Dist(name='distance')\n",
    "    distances = dist_layer(embedding_left, embedding_right)\n",
    "    \n",
    "    classifier = Dense(1, activation='sigmoid')(distances)\n",
    "    \n",
    "    return Model(inputs=[image_left, image_right], outputs=classifier, name='siamese_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b55c0f-fd8c-4d5f-b002-c5bd7c89ac33",
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_model = make_siamese_model()\n",
    "# siamese_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be6fb8e-ab34-4acb-a2a6-2ebe127f3d61",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Train and Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329d9161-48a6-462d-829f-02c35108ec06",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.batch(16).prefetch(tf.data.AUTOTUNE)\n",
    "validation_data = validation_data.batch(16).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2089ff31-3206-4426-aa48-3b094f737fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_cross_loss = tf.losses.BinaryCrossentropy()\n",
    "opt = tf.keras.optimizers.Adam(1e-4) # 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8875f9-68cb-4016-be3e-fb175e25123c",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = 'training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
    "checkpoint = tf.train.Checkpoint(opt=opt, siamese_model=siamese_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3b5233-9037-4f2c-a8f9-9804ddcaf33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('training_results.csv', mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)      \n",
    "    writer.writerow(['Epoch', 'Train Recall', 'Train Precision', 'Train Accuracy', 'Validation Recall', 'Validation Precision', 'Validation Accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef4d3b7-e2f7-4810-9ec9-33b5a3e52152",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(batch):\n",
    "    with tf.GradientTape() as tape:     \n",
    "        X = batch[:2]\n",
    "        y = batch[2] \n",
    "        yhat = siamese_model(X, training=True)\n",
    "        loss = binary_cross_loss(y, yhat)\n",
    "    grad = tape.gradient(loss, siamese_model.trainable_variables)\n",
    "    opt.apply_gradients(zip(grad, siamese_model.trainable_variables))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b5e4c1-d9c4-4f9d-b2a8-1a7bae31c820",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(data):\n",
    "    r = Recall()\n",
    "    p = Precision()\n",
    "    a = BinaryAccuracy()\n",
    "    progbar = tf.keras.utils.Progbar(len(data), unit_name='batch')\n",
    "    for idx, batch in enumerate(data):\n",
    "        yhat = siamese_model.predict(batch[:2], verbose=0)\n",
    "        r.update_state(batch[2], yhat)\n",
    "        p.update_state(batch[2], yhat)\n",
    "        a.update_state(batch[2], yhat)\n",
    "        progbar.update(idx + 1)\n",
    "    return r.result().numpy(), p.result().numpy(), a.result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711715e9-0638-4981-8fc4-b15ad93789ae",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(train_data, validation_data, EPOCHS):\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        print('\\n Epoch {}/{}'.format(epoch, EPOCHS))\n",
    "        progbar = tf.keras.utils.Progbar(len(train_data), unit_name='batch')\n",
    "        r = Recall()\n",
    "        p = Precision()\n",
    "        a = BinaryAccuracy()\n",
    "        for idx, batch in enumerate(train_data):\n",
    "            loss = train_step(batch)\n",
    "            yhat = siamese_model.predict(batch[:2], verbose=0)\n",
    "            r.update_state(batch[2], yhat)\n",
    "            p.update_state(batch[2], yhat)\n",
    "            a.update_state(batch[2], yhat)\n",
    "            progbar.update(idx + 1)\n",
    "        train_recall = r.result().numpy()\n",
    "        train_precision = p.result().numpy()\n",
    "        train_accuracy = a.result().numpy()\n",
    "        print(f'[Train Results] recall: {train_recall} precision: {train_precision} accuracy: {train_accuracy}')\n",
    "        \n",
    "        val_recall, val_precision, val_accuracy = validate(validation_data)\n",
    "        print(f'[Validation Results] recall: {val_recall} precision: {val_precision} accuracy: {val_accuracy}')\n",
    "        \n",
    "        with open('training_results.csv', mode='a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([epoch, train_recall, train_precision, train_accuracy, val_recall, val_precision, val_accuracy])\n",
    "        if epoch % 5 == 0:\n",
    "            checkpoint.save(file_prefix=checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cd2879-c632-4f90-9648-244fd1346939",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train(train_data, validation_data, EPOCHS=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124181d2-a4a0-48f3-ae75-a149411928c6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "siamese_model.save('siamesemodel_v1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2227530-1144-4731-934e-c51b0d8ab44b",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d291717c-4fb4-4615-bbbe-46f41af76113",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data.batch(16).prefetch(tf.data.AUTOTUNE)\n",
    "test_data_men = test_data_men.batch(16).prefetch(tf.data.AUTOTUNE)\n",
    "test_data_women = test_data_women.batch(16).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc67aef5-b0a7-427b-b041-115bade430f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_model = tf.keras.models.load_model('siamesemodel_v1.h5', \n",
    "                                   custom_objects={'L1Dist':L1Dist, 'BinaryCrossentropy':tf.losses.BinaryCrossentropy})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9949e6d6-eb57-4a1b-b375-f2a361c88ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_results.csv', mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)      \n",
    "    writer.writerow(['Gender', 'Test Recall', 'Test Precision', 'Test Accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba23a17-bd85-4003-aaf2-cb48f0ea412f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(data, gender):\n",
    "    r = Recall()\n",
    "    p = Precision()\n",
    "    a = BinaryAccuracy()\n",
    "    for x_left, x_right, y_true in data.as_numpy_iterator():\n",
    "        yhat = siamese_model.predict([x_left, x_right], verbose=0)\n",
    "        r.update_state(y_true, yhat)\n",
    "        p.update_state(y_true, yhat)\n",
    "        a.update_state(y_true, yhat)\n",
    "    test_recall = r.result().numpy()\n",
    "    test_precision = p.result().numpy()\n",
    "    test_accuracy = a.result().numpy()\n",
    "    print(f'|Test Results - {gender}| recall: {test_recall} precision: {test_precision} accuracy: {test_accuracy}')\n",
    "    with open('test_results.csv', mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)      \n",
    "        writer.writerow([gender ,test_recall, test_precision, test_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5c0044-7d7c-42bd-bbd5-231d4ddbb5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test(test_data, \"mixed\")\n",
    "test(test_data_men, \"men\")\n",
    "test(test_data_women, \"women\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
